# -*- coding: utf-8 -*-
"""Projekt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n5UJIHFF6YkdZEaVVEwv9xWsyh_w88UD

Ivon Saldivar  
A02064927
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import pandas as pd
from sklearn import preprocessing
import matplotlib.pyplot as plt

train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')

test_data.head()

all_features = pd.concat((train_data.iloc[:, 2:-1], test_data.iloc[:, 1:-1]))
all_features['Pclass'] = all_features['Pclass'].apply(str)

#del all_features['Cabin']
del all_features['Name']
del all_features['Ticket']

print(all_features.shape)
#print(all_features.to_string())

numeric_data = all_features.select_dtypes(exclude=['object'])
categoric_data = all_features.select_dtypes(include=['object'])#.fillna(0)


#Rescale with zero mean and unit variance
numeric_data = numeric_data.replace(to_replace=np.nan, value=0)
numeric_data = (numeric_data - numeric_data.mean())/numeric_data.std()


#categoric_data.loc[categoric_data['Cabin'] != 0]
categoric_data.loc[:, 'Cabin'] = categoric_data.loc[:,'Cabin'].str.slice(0,1)

all_features = pd.concat([numeric_data, categoric_data], axis=1)


print(all_features)

all_features = all_features.fillna(0)
print(all_features)

all_features = pd.get_dummies(all_features, dummy_na=True)

print(all_features.shape)
all_features.head()

"""Organize the data so we can use it"""

n_train = train_data.shape[0]
train_features = all_features[:n_train].values
test_features = all_features[n_train:].values
train_labels = train_data.Survived.values.reshape((-1, 1))

# I don't know why, but doing "if train_labels == 1:" resulted in everything becoming -10
for i in range(train_labels.shape[0]):
  if train_labels[i] > .5:
    train_labels[i] = 10
  else:
    train_labels[i] = -10

def histogram(data):
  plt.hist(data)
  plt.xlabel('Dead vs Survived')
  plt.ylabel('Quantity')
  plt.show()

"""**Randomly generated results**"""

random_results = pd.DataFrame(np.random.randint(0,2,size=(test_data.shape[0], 1)), columns=["Survived"])
random_results.hist()
random_submission = pd.concat([test_data['PassengerId'], random_results], axis=1)
random_submission.to_csv('random_submission.csv', index=False)
print(random_submission)

"""**Basic regression model**"""

import torch.nn as nn
import torch
from torch.autograd import Variable

epochs = 25
inputDim = train_features.shape[1]
outputDim = 1

class BasicRegression(nn.Module):
    def __init__(self, inputSize, outputSize):
      super(BasicRegression, self).__init__()
      self.linear = nn.Linear(inputSize, outputSize)

    def forward(self, x):
        out = self.linear(x)
        return out



basic_learningRate = 0.01 



basic_regression_model = BasicRegression(inputDim, outputDim)

basic_criterion = nn.MSELoss()
basic_optimizer = torch.optim.SGD(basic_regression_model.parameters(), lr=basic_learningRate)

basic_loss = []

def train_basic_regression():
    for epoch in range(epochs):
    # Converting inputs and labels to Variable
      inputs = Variable(torch.from_numpy(train_features)).float()
      labels = Variable(torch.from_numpy(train_labels)).float()


      # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients
      basic_optimizer.zero_grad()

      # get output from the model, given the inputs
      outputs = basic_regression_model(inputs)

      # get loss for the predicted output
      loss = basic_criterion(outputs, labels)
      #print(loss)
      # get gradients w.r.t to parameters
      loss.backward()

      # update parameters
      basic_optimizer.step()
      
      basic_loss.append(loss.item() / n_train)
      #print('epoch {}, loss {}'.format(epoch, loss.item()))

train_basic_regression()

# evaluate test error

basic_regression_predictions = basic_regression_model(Variable(torch.from_numpy(test_features)).float())

histogram(basic_regression_predictions.detach().numpy())



split = 0
basic_regression_predictions[basic_regression_predictions > split] = 1
basic_regression_predictions[basic_regression_predictions <= split] = 0
basic_regression_predictions = basic_regression_predictions.int()

histogram(basic_regression_predictions.numpy())

basic_regression_results = pd.DataFrame(basic_regression_predictions.numpy(), columns=["Survived"])
basic_regression_results = pd.concat([test_data['PassengerId'], basic_regression_results], axis=1)
basic_regression_results.to_csv('basic_regression_results.csv', index=False)
print(basic_regression_results)

class DeepRegression(nn.Module):
    def __init__(self, inputSize, outputSize):
      super(DeepRegression, self).__init__()
      self.linear = nn.Linear(inputSize, outputSize)
      h = int(inputSize / 2) + outputSize
      self.model = nn.Sequential(
          nn.Linear(inputSize, h),
          nn.Tanh(),
          nn.Linear(h, h),
          nn.Tanh(),
          nn.Linear(h, h),
          nn.Tanh(),
          nn.Linear(h, h),
          nn.Tanh(),
          nn.Linear(h, outputSize),
          nn.Sigmoid()
      )

    def forward(self, x):
        out = self.model(x)
        return out



deep_learningRate = 0.1 


deep_loss = []

deep_regression_model = DeepRegression(inputDim, outputDim)

deep_criterion = nn.MSELoss()
deep_optimizer = torch.optim.SGD(deep_regression_model.parameters(), lr=deep_learningRate)

def train_deep_regression():
    for epoch in range(epochs):
    # Converting inputs and labels to Variable
      inputs = Variable(torch.from_numpy(train_features)).float()
      labels = Variable(torch.from_numpy(train_labels)).float()


      # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients
      deep_optimizer.zero_grad()

      # get output from the model, given the inputs
      outputs = deep_regression_model(inputs)

      # get loss for the predicted output
      loss = deep_criterion(outputs, labels)
      #print(loss)
      # get gradients w.r.t to parameters
      loss.backward()

      # update parameters
      deep_optimizer.step()

      deep_loss.append(loss.item() / n_train)

      #print('epoch {}, loss {}'.format(epoch, loss.item()))


# evaluate test error
train_deep_regression()
deep_regression_predictions = deep_regression_model(Variable(torch.from_numpy(test_features)).float())


histogram(deep_regression_predictions.detach().numpy())


#split = deep_regression_predictions.mean()
split = 0.5
deep_regression_predictions[deep_regression_predictions > split] = 1
deep_regression_predictions[deep_regression_predictions <= split] = 0
deep_regression_predictions = deep_regression_predictions.int()


histogram(deep_regression_predictions.numpy())



deep_regression_results = pd.DataFrame(deep_regression_predictions.numpy(), columns=["Survived"])
deep_regression_results = pd.concat([test_data['PassengerId'], deep_regression_results], axis=1)
deep_regression_results.to_csv('deep_regression_results.csv', index=False)
print(deep_regression_results)

fig = plt.figure(figsize=(6, 4))

epochs_basic = range(len(basic_loss))
epochs_deep = range(len(deep_loss))


line0, = plt.plot(epochs_basic, basic_loss, '--b', LineWidth=2)
line1, = plt.plot(epochs_deep, deep_loss, '--r', LineWidth=2)
plt.xlabel('Epochs', FontSize=20)
plt.ylabel('Loss', FontSize=20)
plt.xticks(FontSize=16)
plt.yticks(FontSize=16)
plt.legend([line0, line1], ['Basic', '7-Layer'], fontsize=20)
plt.tight_layout()
plt.show()
fig.savefig('compare_basic_deep.pdf', format='pdf', dpi=1200)









